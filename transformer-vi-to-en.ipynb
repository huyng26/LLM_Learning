{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3470456,"sourceType":"datasetVersion","datasetId":2089255},{"sourceId":303980,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":259463,"modelId":280647}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nguyncnghuy/transformer-vi-to-en?scriptVersionId=230037923\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install torchtext==0.17.1 numpy==1.26.4 underthesea sacrebleu","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torch==2.2.1 torchtext==0.17.1 torchvision==0.17.1 transformers==4.47.0 evaluate --force-reinstall --no-deps","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\n\n# Configure PyTorch memory allocation\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n# Clear cache\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re,string\nfrom underthesea import word_tokenize\nimport torch\nimport torch.nn as nn\nimport math\nimport warnings\nimport random\nimport evaluate\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:01:19.726369Z","iopub.execute_input":"2025-03-27T15:01:19.726706Z","iopub.status.idle":"2025-03-27T15:01:27.154518Z","shell.execute_reply.started":"2025-03-27T15:01:19.726673Z","shell.execute_reply":"2025-03-27T15:01:27.15352Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## Text Preprocessing","metadata":{}},{"cell_type":"code","source":"import pandas as pd\ntrain_en_path = \"/kaggle/input/iwslt15-englishvietnamese/IWSLT'15 en-vi/train.en.txt\"\ntrain_vi_path = \"/kaggle/input/iwslt15-englishvietnamese/IWSLT'15 en-vi/train.vi.txt\"\n\nwith open(train_en_path, 'r', encoding='utf-8') as f:\n    train_en_lines = f.read().strip().splitlines()\n\nwith open(train_vi_path, 'r', encoding='utf-8') as f:\n    train_vi_lines = f.read().strip().splitlines()\n\n\n\n\n# Create a pandas DataFrame and then a Hugging Face Dataset.\ndata_set = pd.DataFrame({\"en\": train_en_lines, \"vi\": train_vi_lines}).iloc[:100000]\nprint(\"Number of training examples:\", len(data_set))\nprint(\"Sample training data:\")\nprint(data_set.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:01:31.298385Z","iopub.execute_input":"2025-03-27T15:01:31.299277Z","iopub.status.idle":"2025-03-27T15:01:31.656046Z","shell.execute_reply.started":"2025-03-27T15:01:31.299238Z","shell.execute_reply":"2025-03-27T15:01:31.654946Z"}},"outputs":[{"name":"stdout","text":"Number of training examples: 100000\nSample training data:\n                                                  en  \\\n0  Rachel Pike : The science behind a climate hea...   \n1  In 4 minutes , atmospheric chemist Rachel Pike...   \n2  I &apos;d like to talk to you today about the ...   \n3  Headlines that look like this when they have t...   \n4  They are both two branches of the same field o...   \n\n                                                  vi  \n0           Khoa học đằng sau một tiêu đề về khí hậu  \n1  Trong 4 phút , chuyên gia hoá học khí quyển Ra...  \n2  Tôi muốn cho các bạn biết về sự to lớn của nhữ...  \n3  Có những dòng trông như thế này khi bàn về biế...  \n4  Cả hai đều là một nhánh của cùng một lĩnh vực ...  \n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"def preprocessing(df): \n  df[\"en\"] = df[\"en\"].apply(lambda ele: ele.translate(str.maketrans('', '', string.punctuation))) # Remove punctuation\n  df[\"vi\"] = df[\"vi\"].apply(lambda ele: ele.translate(str.maketrans('', '', string.punctuation)))  \n  df[\"en\"] = df[\"en\"].apply(lambda ele: ele.lower()) # convert text to lowercase\n  df[\"vi\"] = df[\"vi\"].apply(lambda ele: ele.lower())\n  df[\"en\"] = df[\"en\"].apply(lambda ele: ele.strip()) \n  df[\"vi\"] = df[\"vi\"].apply(lambda ele: ele.strip()) \n  df[\"en\"] = df[\"en\"].apply(lambda ele: re.sub(\"\\s+\", \" \", ele)) \n  df[\"vi\"] = df[\"vi\"].apply(lambda ele: re.sub(\"\\s+\", \" \", ele))\n    \n  return df\n\ndata_set = preprocessing(data_set)\ndata_set.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:01:50.050153Z","iopub.execute_input":"2025-03-27T15:01:50.050464Z","iopub.status.idle":"2025-03-27T15:01:53.235473Z","shell.execute_reply.started":"2025-03-27T15:01:50.050441Z","shell.execute_reply":"2025-03-27T15:01:53.234445Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                                  en  \\\n0  rachel pike the science behind a climate headline   \n1  in 4 minutes atmospheric chemist rachel pike p...   \n2  i aposd like to talk to you today about the sc...   \n3  headlines that look like this when they have t...   \n4  they are both two branches of the same field o...   \n\n                                                  vi  \n0           khoa học đằng sau một tiêu đề về khí hậu  \n1  trong 4 phút chuyên gia hoá học khí quyển rach...  \n2  tôi muốn cho các bạn biết về sự to lớn của nhữ...  \n3  có những dòng trông như thế này khi bàn về biế...  \n4  cả hai đều là một nhánh của cùng một lĩnh vực ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>en</th>\n      <th>vi</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>rachel pike the science behind a climate headline</td>\n      <td>khoa học đằng sau một tiêu đề về khí hậu</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>in 4 minutes atmospheric chemist rachel pike p...</td>\n      <td>trong 4 phút chuyên gia hoá học khí quyển rach...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>i aposd like to talk to you today about the sc...</td>\n      <td>tôi muốn cho các bạn biết về sự to lớn của nhữ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>headlines that look like this when they have t...</td>\n      <td>có những dòng trông như thế này khi bàn về biế...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>they are both two branches of the same field o...</td>\n      <td>cả hai đều là một nhánh của cùng một lĩnh vực ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_df, test_valid_df = train_test_split(data_set, test_size = 0.2, random_state = 42, shuffle = True)\ntest_df, valid_df = train_test_split(test_valid_df, test_size = 0.5, random_state = 42)\nprint(f\"Train : {len(train_df)}, Valid: {len(valid_df)}, Test: {len(test_df)}, \")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:01:53.236826Z","iopub.execute_input":"2025-03-27T15:01:53.237108Z","iopub.status.idle":"2025-03-27T15:01:53.260562Z","shell.execute_reply.started":"2025-03-27T15:01:53.237086Z","shell.execute_reply":"2025-03-27T15:01:53.259576Z"}},"outputs":[{"name":"stdout","text":"Train : 80000, Valid: 10000, Test: 10000, \n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from datasets import Dataset\ntrain_data = Dataset.from_pandas(train_df)\nvalid_data = Dataset.from_pandas(valid_df)\ntest_data = Dataset.from_pandas(test_df)\ntrain_data[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:01:53.261653Z","iopub.execute_input":"2025-03-27T15:01:53.261905Z","iopub.status.idle":"2025-03-27T15:01:53.57145Z","shell.execute_reply.started":"2025-03-27T15:01:53.261886Z","shell.execute_reply":"2025-03-27T15:01:53.57058Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"{'en': 'so you can see delta 1252 going from kansas city to atlanta',\n 'vi': 'như các bạn có thể thấy đây là kí hiệu chuyến bay delta 1252 đi từ thành phố kansas tới atlanta',\n '__index_level_0__': 75220}"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"en_tokenizer = get_tokenizer('basic_english')\ndef vi_tokenizer(sentence):\n    tokens = word_tokenize(sentence)\n    return tokens\nvi_tokenizer = get_tokenizer(vi_tokenizer)\nprint(en_tokenizer(train_data[0][\"en\"]))\nprint(vi_tokenizer(train_data[0][\"vi\"]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:01:55.048246Z","iopub.execute_input":"2025-03-27T15:01:55.048556Z","iopub.status.idle":"2025-03-27T15:01:55.234064Z","shell.execute_reply.started":"2025-03-27T15:01:55.048531Z","shell.execute_reply":"2025-03-27T15:01:55.233042Z"}},"outputs":[{"name":"stdout","text":"['so', 'you', 'can', 'see', 'delta', '1252', 'going', 'from', 'kansas', 'city', 'to', 'atlanta']\n['như', 'các', 'bạn', 'có thể', 'thấy', 'đây', 'là', 'kí hiệu', 'chuyến', 'bay', 'delta', '1252', 'đi', 'từ', 'thành phố', 'kansas', 'tới', 'atlanta']\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"def tokenize_example(example, en_tokenizer, vi_tokenizer, max_length, sos_token, eos_token):\n    en_tokens = en_tokenizer(example[\"en\"][:max_length])\n    vi_tokens = vi_tokenizer(example[\"vi\"][:max_length])\n    en_tokens = [sos_token] + en_tokens + [eos_token]\n    vi_tokens = [sos_token] + vi_tokens + [eos_token]\n    return {\"en_tokens\":en_tokens, \"vi_tokens\":vi_tokens}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:01:55.814167Z","iopub.execute_input":"2025-03-27T15:01:55.814624Z","iopub.status.idle":"2025-03-27T15:01:55.820769Z","shell.execute_reply.started":"2025-03-27T15:01:55.814571Z","shell.execute_reply":"2025-03-27T15:01:55.819851Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"sos_token = \"<sos>\"\neos_token = \"<eos>\"\nmax_length = 100\nfn_kwargs = {\"en_tokenizer\" : en_tokenizer, \n             \"vi_tokenizer\": vi_tokenizer, \n             \"max_length\":max_length, \"sos_token\" : sos_token, \"eos_token\": eos_token}\ntrain_data = train_data.map(tokenize_example, fn_kwargs = fn_kwargs)\nvalid_data = valid_data.map(tokenize_example, fn_kwargs = fn_kwargs)\ntest_data = test_data.map(tokenize_example, fn_kwargs = fn_kwargs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:02:58.833993Z","iopub.execute_input":"2025-03-27T15:02:58.834311Z","iopub.status.idle":"2025-03-27T15:04:49.409782Z","shell.execute_reply.started":"2025-03-27T15:02:58.834287Z","shell.execute_reply":"2025-03-27T15:04:49.4053Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/80000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9330070bb984972997fb33142444c3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e51fb8032034802b963ea5306ca41e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88e175346ffd47c9a31b596164bb7ace"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"min_freq = 3\nunk_token = \"<unk>\"\npad_token = \"<pad>\"\n\nspecial_tokens = [\n    unk_token,\n    pad_token,\n    sos_token,\n    eos_token,\n]\n\nen_vocab = build_vocab_from_iterator(\n    train_data[\"en_tokens\"],\n    min_freq=min_freq,\n    specials=special_tokens,\n)\n\nvi_vocab = build_vocab_from_iterator(\n    train_data[\"vi_tokens\"],\n    min_freq=min_freq,\n    specials=special_tokens,\n)\nprint(f\"English vocab size: {len(en_vocab)}\")\nprint(f\"Vietnamese vocab size: {len(vi_vocab)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:04:49.412223Z","iopub.execute_input":"2025-03-27T15:04:49.412522Z","iopub.status.idle":"2025-03-27T15:04:53.246565Z","shell.execute_reply.started":"2025-03-27T15:04:49.412496Z","shell.execute_reply":"2025-03-27T15:04:53.245599Z"}},"outputs":[{"name":"stdout","text":"English vocab size: 14754\nVietnamese vocab size: 10963\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"unk_index = en_vocab[\"<unk>\"]\npad_index = en_vocab[\"<pad>\"]\nen_vocab.set_default_index(unk_index)\nvi_vocab.set_default_index(pad_index)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:04:53.248362Z","iopub.execute_input":"2025-03-27T15:04:53.24867Z","iopub.status.idle":"2025-03-27T15:04:53.253891Z","shell.execute_reply.started":"2025-03-27T15:04:53.248643Z","shell.execute_reply":"2025-03-27T15:04:53.252653Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def numericalize_example(example, en_vocab, vi_vocab):\n    en_ids = en_vocab.lookup_indices(example[\"en_tokens\"])\n    vi_ids = vi_vocab.lookup_indices(example[\"vi_tokens\"])\n    return {\"en_ids\":en_ids, \"vi_ids\":vi_ids}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:04:53.255171Z","iopub.execute_input":"2025-03-27T15:04:53.255478Z","iopub.status.idle":"2025-03-27T15:04:53.266558Z","shell.execute_reply.started":"2025-03-27T15:04:53.255444Z","shell.execute_reply":"2025-03-27T15:04:53.265657Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"train_data = train_data.map(numericalize_example, fn_kwargs = {\"en_vocab\": en_vocab, \"vi_vocab\": vi_vocab})\ntest_data = test_data.map(numericalize_example, fn_kwargs = {\"en_vocab\": en_vocab, \"vi_vocab\": vi_vocab})\nvalid_data = valid_data.map(numericalize_example, fn_kwargs = {\"en_vocab\": en_vocab, \"vi_vocab\": vi_vocab})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:04:53.267632Z","iopub.execute_input":"2025-03-27T15:04:53.268123Z","iopub.status.idle":"2025-03-27T15:05:07.638663Z","shell.execute_reply.started":"2025-03-27T15:04:53.26809Z","shell.execute_reply":"2025-03-27T15:05:07.637668Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/80000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f777aaa7f0dd472d8452796641f94ee6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"516c41fad8ed4cfd8ac5b9de99f0146c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"543d32412eb84014accd695a6d9132c2"}},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"vi_vocab.lookup_tokens(train_data[0][\"vi_ids\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:05:07.639545Z","iopub.execute_input":"2025-03-27T15:05:07.639865Z","iopub.status.idle":"2025-03-27T15:05:07.645292Z","shell.execute_reply.started":"2025-03-27T15:05:07.639839Z","shell.execute_reply":"2025-03-27T15:05:07.644617Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"['<sos>',\n 'như',\n 'các',\n 'bạn',\n 'có thể',\n 'thấy',\n 'đây',\n 'là',\n 'kí hiệu',\n 'chuyến',\n 'bay',\n 'delta',\n '<pad>',\n 'đi',\n 'từ',\n 'thành phố',\n 'kansas',\n 'tới',\n 'atlanta',\n '<eos>']"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"train_data = train_data.with_format(type = \"torch\", columns = [\"en_ids\", \"vi_ids\"])\nvalid_data = valid_data.with_format(type = \"torch\", columns = [\"en_ids\", \"vi_ids\"])\ntest_data = test_data.with_format(type = \"torch\", columns = [\"en_ids\", \"vi_ids\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:05:07.646061Z","iopub.execute_input":"2025-03-27T15:05:07.646344Z","iopub.status.idle":"2025-03-27T15:05:07.658443Z","shell.execute_reply.started":"2025-03-27T15:05:07.646312Z","shell.execute_reply":"2025-03-27T15:05:07.657775Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def get_collate_fn(pad_index):\n    def collate_fn(batch):\n        batch_en_ids = [example[\"en_ids\"] for example in batch]\n        batch_vi_ids = [example[\"vi_ids\"] for example in batch]\n        batch_en_ids = nn.utils.rnn.pad_sequence(batch_en_ids, padding_value=pad_index, batch_first = True)\n        batch_vi_ids = nn.utils.rnn.pad_sequence(batch_vi_ids, padding_value=pad_index, batch_first = True)\n        batch = {\n            \"en_ids\": batch_en_ids,\n            \"vi_ids\": batch_vi_ids,\n        }\n        return batch\n\n    return collate_fn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:05:07.660854Z","iopub.execute_input":"2025-03-27T15:05:07.661097Z","iopub.status.idle":"2025-03-27T15:05:07.671234Z","shell.execute_reply.started":"2025-03-27T15:05:07.661074Z","shell.execute_reply":"2025-03-27T15:05:07.670434Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nbatch_size = 32\n\ntrain_data_loader = DataLoader(train_data, batch_size = batch_size , collate_fn = get_collate_fn(pad_index), shuffle = True)\nvalid_data_loader = DataLoader(valid_data, batch_size = batch_size , collate_fn = get_collate_fn(pad_index), shuffle = True)\ntest_data_loader = DataLoader(test_data, batch_size = batch_size , collate_fn = get_collate_fn(pad_index), shuffle = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:05:07.672675Z","iopub.execute_input":"2025-03-27T15:05:07.672949Z","iopub.status.idle":"2025-03-27T15:05:07.681681Z","shell.execute_reply.started":"2025-03-27T15:05:07.672926Z","shell.execute_reply":"2025-03-27T15:05:07.680879Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"## Encoder layer\n- Use positional embedding instead of positional encoding like in the original paper","metadata":{}},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, \n                 input_dim, \n                 hid_dim, \n                 n_layers, \n                 n_heads, \n                 pf_dim,\n                 dropout, \n                 device,\n                 max_length = 100):\n        super().__init__()\n\n        self.device = device\n        \n        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n        \n        self.layers = nn.ModuleList([EncoderLayer(hid_dim, \n                                                  n_heads, \n                                                  pf_dim,\n                                                  dropout, \n                                                  device) \n                                     for _ in range(n_layers)])\n        \n        self.dropout = nn.Dropout(dropout)\n        \n        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n        \n    def forward(self, src, src_mask):\n        \n        #src = [batch size, src len]\n        #src_mask = [batch size, 1, 1, src len]\n        \n        batch_size = src.shape[0]\n        src_len = src.shape[1]\n        \n        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(device)\n        \n        #pos = [batch size, src len]\n        \n        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n        \n        #src = [batch size, src len, hid dim]\n        \n        for layer in self.layers:\n            src = layer(src, src_mask)\n            \n        #src = [batch size, src len, hid dim]\n            \n        return src","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:05:07.682429Z","iopub.execute_input":"2025-03-27T15:05:07.682664Z","iopub.status.idle":"2025-03-27T15:05:07.691133Z","shell.execute_reply.started":"2025-03-27T15:05:07.682643Z","shell.execute_reply":"2025-03-27T15:05:07.690352Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"class EncoderLayer(nn.Module):\n    def __init__(self, \n                 hid_dim, \n                 n_heads, \n                 pf_dim,  \n                 dropout, \n                 device):\n        super().__init__()\n        \n        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n                                                                     pf_dim, \n                                                                     dropout)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, src, src_mask):\n        \n        #src = [batch size, src len, hid dim]\n        #src_mask = [batch size, 1, 1, src len] \n                \n        #self attention\n        _src, _ = self.self_attention(src, src, src, src_mask)\n        \n        #dropout, residual connection and layer norm\n        src = self.self_attn_layer_norm(src + self.dropout(_src))\n        \n        #src = [batch size, src len, hid dim]\n        \n        #positionwise feedforward\n        _src = self.positionwise_feedforward(src)\n        \n        #dropout, residual and layer norm\n        src = self.ff_layer_norm(src + self.dropout(_src))\n        \n        #src = [batch size, src len, hid dim]\n        \n        return src","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:05:07.691966Z","iopub.execute_input":"2025-03-27T15:05:07.692203Z","iopub.status.idle":"2025-03-27T15:05:07.703762Z","shell.execute_reply.started":"2025-03-27T15:05:07.692171Z","shell.execute_reply":"2025-03-27T15:05:07.70295Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"## Multihead attention","metadata":{}},{"cell_type":"code","source":"class MultiHeadAttentionLayer(nn.Module):\n    def __init__(self, hid_dim, n_heads, dropout, device):\n        super().__init__()\n        \n        assert hid_dim % n_heads == 0\n        \n        self.hid_dim = hid_dim\n        self.n_heads = n_heads\n        self.head_dim = hid_dim // n_heads\n        \n        self.fc_q = nn.Linear(hid_dim, hid_dim)\n        self.fc_k = nn.Linear(hid_dim, hid_dim)\n        self.fc_v = nn.Linear(hid_dim, hid_dim)\n        \n        self.fc_o = nn.Linear(hid_dim, hid_dim)\n        \n        self.dropout = nn.Dropout(dropout)\n        \n        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n        \n    def forward(self, query, key, value, mask = None):\n        \n        batch_size = query.shape[0]\n        #query = [batch size, query len, hid dim]\n        #key = [batch size, key len, hid dim]\n        #value = [batch size, value len, hid dim]\n              \n        Q = self.fc_q(query)\n        K = self.fc_k(key)\n        V = self.fc_v(value)\n      \n        #Q = [batch size, query len, hid dim]\n        #K = [batch size, key len, hid dim]\n        #V = [batch size, value len, hid dim]\n                \n        Q = Q.reshape(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n        K = K.reshape(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n        V = V.reshape(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n        \n        #Q = [batch size, n heads, query len, head dim]\n        #K = [batch size, n heads, key len, head dim]\n        #V = [batch size, n heads, value len, head dim]\n        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n        \n        #energy = [batch size, n heads, query len, key len]\n        \n        if mask is not None:\n            energy = energy.masked_fill(mask == 0, -1e10)\n        \n        attention = torch.softmax(energy, dim = -1)\n                \n        #attention = [batch size, n heads, query len, key len]\n                \n        x = torch.matmul(self.dropout(attention), V)\n        \n        #x = [batch size, n heads, query len, head dim]\n        \n        x = x.permute(0, 2, 1, 3).contiguous()\n        \n        #x = [batch size, query len, n heads, head dim]\n        \n        x = x.view(batch_size, -1, self.hid_dim)\n        \n        #x = [batch size, query len, hid dim]\n        \n        x = self.fc_o(x)\n        \n        #x = [batch size, query len, hid dim]\n        \n        return x, attention","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:05:07.704459Z","iopub.execute_input":"2025-03-27T15:05:07.704646Z","iopub.status.idle":"2025-03-27T15:05:07.720883Z","shell.execute_reply.started":"2025-03-27T15:05:07.704629Z","shell.execute_reply":"2025-03-27T15:05:07.720143Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"class PositionwiseFeedforwardLayer(nn.Module):\n    def __init__(self, hid_dim, pf_dim, dropout):\n        super().__init__()\n        \n        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n        \n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        \n        #x = [batch size, seq len, hid dim]\n        \n        x = self.dropout(torch.relu(self.fc_1(x)))\n        \n        #x = [batch size, seq len, pf dim]\n        \n        x = self.fc_2(x)\n        \n        #x = [batch size, seq len, hid dim]\n        \n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:05:07.721602Z","iopub.execute_input":"2025-03-27T15:05:07.721841Z","iopub.status.idle":"2025-03-27T15:05:07.735174Z","shell.execute_reply.started":"2025-03-27T15:05:07.721821Z","shell.execute_reply":"2025-03-27T15:05:07.734295Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"## Decoder layer","metadata":{}},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, \n                 output_dim, \n                 hid_dim, \n                 n_layers, \n                 n_heads, \n                 pf_dim, \n                 dropout, \n                 device,\n                 max_length = 100):\n        super().__init__()\n        \n        self.device = device\n        \n        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n        \n        self.layers = nn.ModuleList([DecoderLayer(hid_dim, \n                                                  n_heads, \n                                                  pf_dim, \n                                                  dropout, \n                                                  device)\n                                     for _ in range(n_layers)])\n        \n        self.fc_out = nn.Linear(hid_dim, output_dim)\n        \n        self.dropout = nn.Dropout(dropout)\n        \n        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n        \n    def forward(self, trg, enc_src, trg_mask, src_mask):\n        \n        #trg = [batch size, trg len]\n        #enc_src = [batch size, src len, hid dim]\n        #trg_mask = [batch size, 1, trg len, trg len]\n        #src_mask = [batch size, 1, 1, src len]\n                \n        batch_size = trg.shape[0]\n        trg_len = trg.shape[1]\n        \n        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n                            \n        #pos = [batch size, trg len]\n            \n        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n                \n        #trg = [batch size, trg len, hid dim]\n        \n        for layer in self.layers:\n            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n        \n        #trg = [batch size, trg len, hid dim]\n        #attention = [batch size, n heads, trg len, src len]\n        \n        output = self.fc_out(trg)\n        \n        #output = [batch size, trg len, output dim]\n            \n        return output, attention","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:05:07.735823Z","iopub.execute_input":"2025-03-27T15:05:07.736039Z","iopub.status.idle":"2025-03-27T15:05:07.746969Z","shell.execute_reply.started":"2025-03-27T15:05:07.736021Z","shell.execute_reply":"2025-03-27T15:05:07.746272Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"class DecoderLayer(nn.Module):\n    def __init__(self, \n                 hid_dim, \n                 n_heads, \n                 pf_dim, \n                 dropout, \n                 device):\n        super().__init__()\n        \n        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n                                                                     pf_dim, \n                                                                     dropout)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, trg, enc_src, trg_mask, src_mask):\n        \n        #trg = [batch size, trg len, hid dim]\n        #enc_src = [batch size, src len, hid dim]\n        #trg_mask = [batch size, 1, trg len, trg len]\n        #src_mask = [batch size, 1, 1, src len]\n        \n        #self attention\n        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n        \n        #dropout, residual connection and layer norm\n        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n            \n        #trg = [batch size, trg len, hid dim]\n            \n        #encoder attention\n        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n        \n        #dropout, residual connection and layer norm\n        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n                    \n        #trg = [batch size, trg len, hid dim]\n        \n        #positionwise feedforward\n        _trg = self.positionwise_feedforward(trg)\n        \n        #dropout, residual and layer norm\n        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n        \n        #trg = [batch size, trg len, hid dim]\n        #attention = [batch size, n heads, trg len, src len]\n        \n        return trg, attention","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:05:07.747707Z","iopub.execute_input":"2025-03-27T15:05:07.748032Z","iopub.status.idle":"2025-03-27T15:05:07.760322Z","shell.execute_reply.started":"2025-03-27T15:05:07.748Z","shell.execute_reply":"2025-03-27T15:05:07.759688Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"## Tranlation model","metadata":{}},{"cell_type":"code","source":"class Seq2Seq(nn.Module):\n    def __init__(self, \n                 encoder, \n                 decoder, \n                 src_pad_idx, \n                 trg_pad_idx, \n                 device):\n        super().__init__()\n        \n        self.encoder = encoder\n        self.decoder = decoder\n        self.src_pad_idx = src_pad_idx\n        self.trg_pad_idx = trg_pad_idx\n        self.device = device\n        \n    def make_src_mask(self, src):\n        \n        #src = [batch size, src len]\n        \n        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n\n        #src_mask = [batch size, 1, 1, src len]\n\n        return src_mask\n    \n    def make_trg_mask(self, trg):\n        \n        #trg = [batch size, trg len]\n        \n        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n        \n        #trg_pad_mask = [batch size, 1, 1, trg len]\n        \n        trg_len = trg.shape[1]\n        \n        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n        \n        #trg_sub_mask = [trg len, trg len]\n            \n        trg_mask = trg_pad_mask & trg_sub_mask\n        \n        #trg_mask = [batch size, 1, trg len, trg len]\n        \n        return trg_mask\n\n    def forward(self, src, trg):\n        \n        #src = [batch size, src len]\n        #trg = [batch size, trg len]\n                \n        src_mask = self.make_src_mask(src)\n        trg_mask = self.make_trg_mask(trg)\n        \n        #src_mask = [batch size, 1, 1, src len]\n        #trg_mask = [batch size, 1, trg len, trg len]\n        \n        enc_src = self.encoder(src, src_mask)\n        \n        #enc_src = [batch size, src len, hid dim]\n                \n        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n        \n        #output = [batch size, trg len, output dim]\n        #attention = [batch size, n heads, trg len, src len]\n        \n        return output, attention","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:05:07.761154Z","iopub.execute_input":"2025-03-27T15:05:07.761383Z","iopub.status.idle":"2025-03-27T15:05:07.773032Z","shell.execute_reply.started":"2025-03-27T15:05:07.761364Z","shell.execute_reply":"2025-03-27T15:05:07.772138Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"INPUT_DIM = len(en_vocab)\nOUTPUT_DIM = len(vi_vocab)\nHID_DIM = 256\nENC_LAYERS = 3\nDEC_LAYERS = 3\nENC_HEADS = 8\nDEC_HEADS = 8\nENC_PF_DIM = 512\nDEC_PF_DIM = 512\nENC_DROPOUT = 0.1\nDEC_DROPOUT = 0.1\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nenc = Encoder(INPUT_DIM, \n              HID_DIM, \n              ENC_LAYERS, \n              ENC_HEADS, \n              ENC_PF_DIM, \n              ENC_DROPOUT, \n              device)\n\ndec = Decoder(OUTPUT_DIM, \n              HID_DIM, \n              DEC_LAYERS, \n              DEC_HEADS, \n              DEC_PF_DIM, \n              DEC_DROPOUT, \n              device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:05:07.773865Z","iopub.execute_input":"2025-03-27T15:05:07.774157Z","iopub.status.idle":"2025-03-27T15:05:08.074184Z","shell.execute_reply.started":"2025-03-27T15:05:07.774127Z","shell.execute_reply":"2025-03-27T15:05:08.073387Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"model = Seq2Seq(enc, dec, pad_index, pad_index, device).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:05:08.075092Z","iopub.execute_input":"2025-03-27T15:05:08.075334Z","iopub.status.idle":"2025-03-27T15:05:08.099523Z","shell.execute_reply.started":"2025-03-27T15:05:08.075314Z","shell.execute_reply":"2025-03-27T15:05:08.098907Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"def initialize_weights(m):\n    if hasattr(m, 'weight') and m.weight.dim() > 1:\n        nn.init.xavier_uniform_(m.weight.data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:02:48.804156Z","iopub.status.idle":"2025-03-27T15:02:48.804531Z","shell.execute_reply":"2025-03-27T15:02:48.80435Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.apply(initialize_weights)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:02:48.805478Z","iopub.status.idle":"2025-03-27T15:02:48.805882Z","shell.execute_reply":"2025-03-27T15:02:48.805675Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"LEARNING_RATE = 0.0005\noptimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\ncriterion = nn.CrossEntropyLoss(ignore_index=pad_index)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:05:08.100318Z","iopub.execute_input":"2025-03-27T15:05:08.100626Z","iopub.status.idle":"2025-03-27T15:05:08.106221Z","shell.execute_reply.started":"2025-03-27T15:05:08.100594Z","shell.execute_reply":"2025-03-27T15:05:08.105509Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"import tqdm\ndef train_fn(model, iterator, optimizer, criterion, clip, device):\n    \n    model.train()\n    \n    epoch_loss = 0\n    \n    for batch in tqdm.tqdm(iterator):\n        \n        src = batch[\"en_ids\"].to(device)\n        trg = batch[\"vi_ids\"].to(device)\n        \n        optimizer.zero_grad()\n        \n        output, _ = model(src, trg[:,:-1])\n                \n        #output = [batch size, trg len - 1, output dim]\n        #trg = [batch size, trg len]\n            \n        output_dim = output.shape[-1]\n            \n        output = output.contiguous().view(-1, output_dim)\n        trg = trg[:,1:].contiguous().view(-1)\n                \n        #output = [batch size * trg len - 1, output dim]\n        #trg = [batch size * trg len - 1]\n            \n        loss = criterion(output, trg)\n        \n        loss.backward()\n        \n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        \n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        \n    return epoch_loss / len(iterator)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:05:08.107111Z","iopub.execute_input":"2025-03-27T15:05:08.107417Z","iopub.status.idle":"2025-03-27T15:05:08.117563Z","shell.execute_reply.started":"2025-03-27T15:05:08.107384Z","shell.execute_reply":"2025-03-27T15:05:08.116936Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"def evaluate_fn(model, iterator, criterion, device):\n    \n    model.eval()\n    \n    epoch_loss = 0\n    \n    with torch.no_grad():\n    \n        for batch in tqdm.tqdm(iterator):\n\n            src = batch[\"en_ids\"].to(device)\n            trg = batch[\"vi_ids\"].to(device)\n\n            output, _ = model(src, trg[:,:-1])\n            \n            #output = [batch size, trg len - 1, output dim]\n            #trg = [batch size, trg len]\n            \n            output_dim = output.shape[-1]\n            \n            output = output.contiguous().view(-1, output_dim)\n            trg = trg[:,1:].contiguous().view(-1)\n            \n            #output = [batch size * trg len - 1, output dim]\n            #trg = [batch size * trg len - 1]\n            \n            loss = criterion(output, trg)\n\n            epoch_loss += loss.item()\n        \n    return epoch_loss / len(iterator)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:05:08.118384Z","iopub.execute_input":"2025-03-27T15:05:08.118679Z","iopub.status.idle":"2025-03-27T15:05:08.129172Z","shell.execute_reply.started":"2025-03-27T15:05:08.118648Z","shell.execute_reply":"2025-03-27T15:05:08.128419Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"def epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time / 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\nN_EPOCHS = 20\nCLIP = 1\n\nbest_valid_loss = float('inf')\n\nfor epoch in range(N_EPOCHS):\n    \n    start_time = time.time()\n    \n    train_loss = train_fn(model, train_data_loader, optimizer, criterion, CLIP, device)\n    valid_loss = evaluate_fn(model, valid_data_loader, criterion, device)\n    \n    end_time = time.time()\n    \n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n    \n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        model_path = \"/kaggle/working/transformer-model.pt\"\n        torch.save(model.state_dict(), model_path)\n        torch.save(model.state_dict(), 'transformer-model.pt')\n    \n    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nmodel.load_state_dict(torch.load('/kaggle/input/transformer-en-to-vi/pytorch/default/1/transformer-model.pt'))\n\ntest_loss = evaluate_fn(model, test_data_loader, criterion, device)\n\nprint(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:05:08.12988Z","iopub.execute_input":"2025-03-27T15:05:08.130095Z","iopub.status.idle":"2025-03-27T15:05:11.509448Z","shell.execute_reply.started":"2025-03-27T15:05:08.130067Z","shell.execute_reply":"2025-03-27T15:05:11.508449Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 313/313 [00:03<00:00, 95.03it/s] ","output_type":"stream"},{"name":"stdout","text":"| Test Loss: 2.992 | Test PPL:  19.934 |\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"def translate_sentence(\n    sentence,\n    model,\n    en_tokenizer,\n    vi_tokenizer,\n    en_vocab,\n    vi_vocab,\n    lower,\n    sos_token,\n    eos_token,\n    device,\n    max_output_length=50,\n):\n    model.eval()\n    with torch.no_grad():\n        if isinstance(sentence, str):\n            en_tokens = [token for token in en_tokenizer(sentence)]\n        else:\n            en_tokens = [token for token in sentence]\n        if lower:\n            en_tokens = [token.lower() for token in en_tokens]\n        en_tokens = [sos_token] + en_tokens + [eos_token]\n        ids = en_vocab.lookup_indices(en_tokens)\n        src_tensor = torch.LongTensor(ids).unsqueeze(0).to(device)\n        src_mask = model.make_src_mask(src_tensor)\n        with torch.no_grad():\n            enc_src = model.encoder(src_tensor, src_mask)\n        inputs = en_vocab.lookup_indices([sos_token])\n        for i in range(max_output_length):\n            inputs_tensor = torch.LongTensor(inputs).unsqueeze(0).to(device)\n            inputs_mask = model.make_trg_mask(inputs_tensor)\n            with torch.no_grad():\n                output, attention = model.decoder(inputs_tensor, enc_src, inputs_mask, src_mask)\n            pred_token = output.argmax(2)[:,-1].item()\n            inputs.append(pred_token)\n            if pred_token == vi_vocab[eos_token]:\n                break\n        vi_tokens = vi_vocab.lookup_tokens(inputs)\n    return vi_tokens, en_tokens, attention","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:05:11.512508Z","iopub.execute_input":"2025-03-27T15:05:11.512755Z","iopub.status.idle":"2025-03-27T15:05:11.519295Z","shell.execute_reply.started":"2025-03-27T15:05:11.512731Z","shell.execute_reply":"2025-03-27T15:05:11.518458Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\ndef display_attention(sentence, translation, attention, n_heads = 8, n_rows = 4, n_cols = 2):\n    \n    assert n_rows * n_cols == n_heads\n    \n    fig = plt.figure(figsize=(15,25))\n    \n    for i in range(n_heads):\n        \n        ax = fig.add_subplot(n_rows, n_cols, i+1)\n        \n        _attention = attention.squeeze(0)[i].cpu().detach().numpy()\n\n        cax = ax.matshow(_attention, cmap='bone')\n\n        ax.tick_params(labelsize=12)\n        ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'], \n                           rotation=45)\n        ax.set_yticklabels(['']+translation)\n\n        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\n    plt.show()\n    plt.close()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sentence = \"What's your name\"\nlower = True\ntranslation, sentence_tokens, attention = translate_sentence(\n    sentence,\n    model,\n    en_tokenizer,\n    vi_tokenizer,\n    en_vocab,\n    vi_vocab,\n    lower,\n    sos_token,\n    eos_token,\n    device,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:05:11.520453Z","iopub.execute_input":"2025-03-27T15:05:11.520781Z","iopub.status.idle":"2025-03-27T15:05:11.590918Z","shell.execute_reply.started":"2025-03-27T15:05:11.520747Z","shell.execute_reply":"2025-03-27T15:05:11.5902Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"print(f\"Translated sentence: {translation}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:05:11.591916Z","iopub.execute_input":"2025-03-27T15:05:11.592261Z","iopub.status.idle":"2025-03-27T15:05:11.596483Z","shell.execute_reply.started":"2025-03-27T15:05:11.592227Z","shell.execute_reply":"2025-03-27T15:05:11.595438Z"}},"outputs":[{"name":"stdout","text":"Translated sentence: ['<sos>', 'cái', 'tên', 'của', 'bạn', 'là', 'gì', '<eos>']\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"sentence_tokens","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:05:11.597404Z","iopub.execute_input":"2025-03-27T15:05:11.597752Z","iopub.status.idle":"2025-03-27T15:05:11.60807Z","shell.execute_reply.started":"2025-03-27T15:05:11.597685Z","shell.execute_reply":"2025-03-27T15:05:11.607286Z"}},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"['<sos>', 'what', \"'\", 's', 'your', 'name', '<eos>']"},"metadata":{}}],"execution_count":36},{"cell_type":"markdown","source":"## 8 heads , 8 multihead attention","metadata":{}},{"cell_type":"code","source":"display_attention(sentence, translation, attention)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Calculate BLEU","metadata":{}},{"cell_type":"code","source":"test_data = test_data.with_format(type = None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:05:11.608956Z","iopub.execute_input":"2025-03-27T15:05:11.609246Z","iopub.status.idle":"2025-03-27T15:05:11.620773Z","shell.execute_reply.started":"2025-03-27T15:05:11.609215Z","shell.execute_reply":"2025-03-27T15:05:11.619813Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"test_data[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:05:11.621582Z","iopub.execute_input":"2025-03-27T15:05:11.621916Z","iopub.status.idle":"2025-03-27T15:05:11.634553Z","shell.execute_reply.started":"2025-03-27T15:05:11.621886Z","shell.execute_reply":"2025-03-27T15:05:11.633669Z"}},"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"{'en': 'sb this girl was working as a maid before she came to school',\n 'vi': 'em gái này từng làm người hầu trước khi em được đến trường',\n '__index_level_0__': 48540,\n 'en_tokens': ['<sos>',\n  'sb',\n  'this',\n  'girl',\n  'was',\n  'working',\n  'as',\n  'a',\n  'maid',\n  'before',\n  'she',\n  'came',\n  'to',\n  'school',\n  '<eos>'],\n 'vi_tokens': ['<sos>',\n  'em gái',\n  'này',\n  'từng',\n  'làm',\n  'người',\n  'hầu',\n  'trước',\n  'khi',\n  'em',\n  'được',\n  'đến',\n  'trường',\n  '<eos>'],\n 'en_ids': [2, 6609, 17, 517, 21, 242, 40, 8, 0, 185, 100, 187, 6, 217, 3],\n 'vi_ids': [2, 1910, 20, 150, 34, 18, 9734, 103, 33, 293, 21, 46, 282, 3]}"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"sacrebleu = evaluate.load('sacrebleu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:20:17.277451Z","iopub.execute_input":"2025-03-27T15:20:17.277868Z","iopub.status.idle":"2025-03-27T15:20:17.511303Z","shell.execute_reply.started":"2025-03-27T15:20:17.277831Z","shell.execute_reply":"2025-03-27T15:20:17.510331Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"test_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:22:43.952019Z","iopub.execute_input":"2025-03-27T15:22:43.952445Z","iopub.status.idle":"2025-03-27T15:22:43.957843Z","shell.execute_reply.started":"2025-03-27T15:22:43.952414Z","shell.execute_reply":"2025-03-27T15:22:43.957013Z"}},"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['en', 'vi', '__index_level_0__', 'en_tokens', 'vi_tokens', 'en_ids', 'vi_ids'],\n    num_rows: 10000\n})"},"metadata":{}}],"execution_count":52},{"cell_type":"code","source":"translations = [\n    translate_sentence(\n        example[\"en\"][:100],\n        model,\n        en_tokenizer,\n        vi_tokenizer,\n        en_vocab,\n        vi_vocab,\n        lower,\n        sos_token,\n        eos_token,\n        device\n    )[0] for example in tqdm.tqdm(test_data)\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:08:13.654815Z","iopub.execute_input":"2025-03-27T15:08:13.655124Z","iopub.status.idle":"2025-03-27T15:17:19.502327Z","shell.execute_reply.started":"2025-03-27T15:08:13.655102Z","shell.execute_reply":"2025-03-27T15:17:19.501482Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 10000/10000 [09:05<00:00, 18.32it/s]\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"predictions = [\" \".join(translation[1:-1]) for translation in translations]\n\nreferences = [[example[\"vi\"]] for example in test_data]\nprint(len(references))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:28:04.824442Z","iopub.execute_input":"2025-03-27T15:28:04.824937Z","iopub.status.idle":"2025-03-27T15:28:06.066387Z","shell.execute_reply.started":"2025-03-27T15:28:04.824893Z","shell.execute_reply":"2025-03-27T15:28:06.06546Z"}},"outputs":[{"name":"stdout","text":"10000\n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"def get_tokenizer_fn(nlp, lower):\n    def tokenizer_fn(s):\n        tokens = [token for token in nlp(s)]\n        if lower:\n            tokens = [token.lower() for token in tokens]\n        return tokens\n\n    return tokenizer_fn","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer_fn = get_tokenizer_fn(vi_tokenizer, lower)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = sacrebleu.compute(\n    predictions=predictions, references=references)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:28:09.714582Z","iopub.execute_input":"2025-03-27T15:28:09.714934Z","iopub.status.idle":"2025-03-27T15:28:11.785732Z","shell.execute_reply.started":"2025-03-27T15:28:09.714907Z","shell.execute_reply":"2025-03-27T15:28:11.784831Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:28:15.447929Z","iopub.execute_input":"2025-03-27T15:28:15.448276Z","iopub.status.idle":"2025-03-27T15:28:15.454533Z","shell.execute_reply.started":"2025-03-27T15:28:15.448248Z","shell.execute_reply":"2025-03-27T15:28:15.453628Z"}},"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"{'score': 14.954648487544354,\n 'counts': [87803, 44499, 22831, 12212],\n 'totals': [159488, 149488, 139502, 129625],\n 'precisions': [55.05304474317817,\n  29.767606764422563,\n  16.366073604679503,\n  9.421022179363549],\n 'bp': 0.6670129442085727,\n 'sys_len': 159488,\n 'ref_len': 224072}"},"metadata":{}}],"execution_count":59},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Low BLEU score due to small dataset size(100000 pairs)","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}